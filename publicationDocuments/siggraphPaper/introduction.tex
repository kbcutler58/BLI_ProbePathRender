\section{Introduction}

In medicine as well as other areas, it is important to know the location of an object as it is moving across a surface. In our case, we have a Diffuse Optics Spectrograph Imaging (DOSI) probe that shines near-IR light into a patient and measures the absorption and scattering of the returned light. The data from the probe has important medical applications including where tumors are located. Becuase they need to know the exact location of the data points, they attach a grid to the patient and make sure the readings are taken at each point on the grid. The probes however take many readings every second. We thus realized that if we could get the location in an automated matter, not only would they not have to use the grid, but a higher density of readings would be available. \\
\\
In order for this technology to be useful and effective, we need to provide a visualization of the probe's location in real-time. This way the operator can ensure that data is being taken across a vast enough area. This means that we need a method for visualizing the location of the probe on the patient's body. We first need a reference for the patient. In order to achieve this, we take a 3D scan of our patient and load it into a 3D visualization environment. Using a 3D environment provides much more flexibility and detail than if we simply used a 2D picture. Before the scanning is done, we mark calibration points on the surface of the patient. Once the model of the patient has been loaded into our environment, we put the probe at one of the calibration points and start tracking its location. This paper is focused on the algorithms we used to be able to provide an accurate visualization of its location once the tracking started. \\
\\
If the surface we were tracking on were rigid, then this problem would be easy. However, the surface is elastic meaning that it can deform but it retains its shape after pressure is no longer being applied. This means that if we mark a location on the surface while pressure is being applied, then we will consider the true location of that mark to be where it shows up after the pressure is no longer being applied. Using this idea, we want to track the location on the surface where the probe traversed, given that we know the surface will deform when pressure is applied. \\
\\
Because we want to try the location on the surface directly, we decided to attach an optical mouse sensor to the bottom of the probe. This way we get displacement readings on the surface. While this gives us x and y output, we have no way of knowing the orientation of these axes in the real world. For this reason, we attached a compass to the probe. If we were tracking on a flat surface, this would be enough to provide an accuruate location. However, because our surfaces are curved, we also attached a gyroscope and accelerometer to get all 3D rotations of the axes possible. \\
\\
We now had data coming in from displacement and orientation sensors and needed to figure out how to turn this into reliable 3D paths. Since we had orientation and displacement data, we could simply construct a segment for each displacement and make sure that segment is orientated using the sensor data. While this will produce a 3D path, it will likely be much longer or shorter than desired and travel in an undesirable direction in the virtual envornment. This is due to the face that no calibration has been done to reconcile the virtual environment with the real world. Additionally, nothing has been done to ensure the path is on the surface in the virtual environment.\\
\\
In order to ensure that the path is not longer or shorter than desired, there are 3 scales that need to be reconciled: the scale of the virtual environment, the scale of the real world, and the scale that the displacement readings operate at. This ensures that when the displacement readings from the sensors are obtained, we have a multiplier that translate that to an accurate displacement in the virtual environment. In order to ensure that the direction is correct, we need to find a rotation matrix to translate rotation in the real world with rotation in the virtual world. That way we can use that rotation matrix to produce an accurate direction in the virtual world with each movement of the probe. Finally, we need an algorithm to translate the 3D path to a path that is directly on the surface given that we know the surface will be deformable. Once these are achieved, we will be able to provide accurate real-time paths on the surface of a patient. \\
\\
In this paper, we contribute a scale calibration, rotation calibration, and mesh following algorithm that allows us to accomplish the previously mentioned goals. Additionally, using a combination of our rotation calibaration and mesh following algoirthms, we were able to calibrate our device on curved deformable surfaces providing greater flexibility. After calibration was done we were able to provide reasonable accurate and informative visualizations of the location of our probe. When tracking in real-time we were able to achieve an accuracy of **PUT ACCURACY NUMBERS FOR REAL-TIME PATHS**. When these paths were post-processed for longer term uses, we achieved an accuracy of **PUT ACCURACY NUMBERS FOR POST-PROCESSED PATHS**.  